# LLM Rotation Proxy Configuration
# Priority: Lower numbers = higher priority (tried first)

providers:
  - name: "grok"
    priority: 1
    base_url: "https://api.x.ai/v1"
    api_key: "YOUR_GROK_API_KEY_HERE"
    model: "grok-beta"

  - name: "cerebras"
    priority: 2
    base_url: "https://api.cerebras.ai/v1"
    api_key: "YOUR_CEREBRAS_API_KEY_HERE"
    model: "llama3.1-8b"

  - name: "gemini"
    priority: 3
    base_url: "https://generativelanguage.googleapis.com/v1beta/openai"
    api_key: "YOUR_GEMINI_API_KEY_HERE"
    model: "gemini-1.5-flash"

  - name: "openai"
    priority: 4
    base_url: "https://api.openai.com/v1"
    api_key: "YOUR_OPENAI_API_KEY_HERE"
    model: "gpt-3.5-turbo"

  - name: "mistral"
    priority: 5
    base_url: "https://api.mistral.ai/v1"
    api_key: "YOUR_MISTRAL_API_KEY_HERE"
    model: "mistral-small-latest"

  - name: "together"
    priority: 6
    base_url: "https://api.together.xyz/v1"
    api_key: "YOUR_TOGETHER_API_KEY_HERE"
    model: "meta-llama/Llama-3-8b-chat-hf"

# Notes:
# - Remove or comment out providers you don't want to use
# - Adjust priorities as needed (1 = highest priority)
# - The proxy will automatically rotate when rate limits are hit
# - Request counts reset daily for each provider
