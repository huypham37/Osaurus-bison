# LLM Rotation Proxy - Quick Start

Get up and running in 5 minutes!

## What This Does

Automatically rotates between multiple free-tier LLM providers when you hit rate limits. Never pay for LLM subscriptions again!

**Supported providers:**
- Grok (X.AI)
- Cerebras
- Google Gemini
- OpenAI
- Mistral AI
- Together AI

## Installation

### Step 1: Install Dependencies

```bash
cd llm-proxy
./install.sh
```

### Step 2: Add Your API Keys

Edit `config.yaml`:

```yaml
providers:
  - name: "grok"
    priority: 1
    api_key: "YOUR_GROK_API_KEY_HERE"  # ‚Üê Replace this
    # ...
```

Get free API keys from:
- Grok: https://x.ai/api
- Cerebras: https://cloud.cerebras.ai/
- Gemini: https://makersuite.google.com/app/apikey
- OpenAI: https://platform.openai.com/api-keys
- Mistral: https://console.mistral.ai/
- Together: https://api.together.xyz/

**Tip:** You don't need all of them! Just add the ones you have.

### Step 3: Test It

```bash
# Start the proxy
python3 proxy_server.py
```

In another terminal:

```bash
# Run tests
python3 test_proxy.py
```

You should see:
```
‚úì PASS - Health Check
‚úì PASS - Status Endpoint
‚úì PASS - Chat Completion
```

Press Ctrl+C to stop the proxy.

## Integration with Osaurus App

### Quick Integration (3 steps)

1. **Add ProxyManager to your Xcode project:**
   ```
   Drag ProxyManager.swift into Xcode
   ```

2. **Start proxy on app launch:**
   ```swift
   // In AppDelegate.swift
   func applicationDidFinishLaunching() {
       ProxyManager.shared.ensureProxyRunning { success, error in
           if success {
               print("‚úì Ready!")
           }
       }
   }
   ```

3. **Update your API endpoint:**
   ```swift
   // Change from:
   let url = "https://api.openai.com/v1/chat/completions"

   // To:
   let url = "http://localhost:8000/v1/chat/completions"
   ```

**Done!** Your app now automatically rotates providers.

See `INTEGRATION_GUIDE.md` for detailed instructions.

## Usage

### Manual Testing

```bash
# Start the proxy
python3 proxy_server.py
```

The proxy runs on `http://localhost:8000`

### Check Status

```bash
# Health check
curl http://localhost:8000/health

# See which providers are available
curl http://localhost:8000/status
```

### Make a Request

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## How It Works

```
Your request
    ‚Üì
Try Provider 1 (highest priority)
    ‚îú‚îÄ Success? ‚Üí Return response ‚úì
    ‚îî‚îÄ Rate limited (429)? ‚Üí Try Provider 2
            ‚îú‚îÄ Success? ‚Üí Return response ‚úì
            ‚îî‚îÄ Rate limited? ‚Üí Try Provider 3
                    ‚îî‚îÄ ... and so on
```

## Configuration

### Adjust Provider Priority

Lower number = higher priority (tried first):

```yaml
providers:
  - name: "grok"
    priority: 1      # Tried first

  - name: "gemini"
    priority: 2      # Tried second
```

### Disable a Provider

Just comment it out:

```yaml
  # - name: "openai"    # Disabled
  #   priority: 4
```

### Use Different Models

Change the `model` field:

```yaml
  - name: "cerebras"
    model: "llama3.1-70b"  # Use larger model
```

## Monitoring

### View Request Counts

```bash
curl http://localhost:8000/stats | python3 -m json.tool
```

Output:
```json
{
  "providers": [
    {
      "name": "grok",
      "request_count": 42,
      "is_rate_limited": false,
      "last_reset": "2025-11-06"
    }
  ]
}
```

Request counts reset daily.

### Check Rate Limits

```bash
curl http://localhost:8000/status
```

Shows which providers are currently rate-limited and when they'll be available.

## Troubleshooting

### "Connection refused"

The proxy isn't running. Start it:
```bash
python3 proxy_server.py
```

### "All providers unavailable"

All providers are rate-limited. Wait a few minutes or check your API keys:
```bash
curl http://localhost:8000/status
```

### "ModuleNotFoundError"

Install dependencies:
```bash
pip3 install -r requirements.txt
```

## Tips

1. **Prioritize fastest providers first** - They'll give the best user experience
2. **Check free tier limits** - Each provider has different daily limits
3. **Monitor request counts** - Use `/stats` to see which providers you use most
4. **Add all providers** - More providers = less downtime
5. **Test individually** - Verify each API key works before adding to production

## Files Overview

```
llm-proxy/
‚îú‚îÄ‚îÄ proxy_server.py          # Main server (run this)
‚îú‚îÄ‚îÄ providers.py             # Provider management logic
‚îú‚îÄ‚îÄ config.yaml              # Your configuration (edit this)
‚îú‚îÄ‚îÄ ProxyManager.swift       # Swift integration for your app
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ install.sh               # Installation script
‚îú‚îÄ‚îÄ test_proxy.py            # Test script
‚îú‚îÄ‚îÄ README.md                # Full documentation
‚îú‚îÄ‚îÄ INTEGRATION_GUIDE.md     # Detailed integration guide
‚îî‚îÄ‚îÄ QUICKSTART.md            # This file
```

## Next Steps

1. ‚úÖ Install and test the proxy (you just did this!)
2. ‚úÖ Add to your Osaurus app (see INTEGRATION_GUIDE.md)
3. ‚úÖ Test with real API keys
4. ‚úÖ Monitor usage and adjust priorities
5. ‚úÖ Enjoy unlimited LLM access! üéâ

## Need Help?

- Check `README.md` for full documentation
- Check `INTEGRATION_GUIDE.md` for app integration details
- Review code comments in `proxy_server.py` and `providers.py`

Happy coding! üöÄ
